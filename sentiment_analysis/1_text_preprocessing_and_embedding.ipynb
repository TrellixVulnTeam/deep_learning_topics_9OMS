{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing and Word Embeddings\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sentiment_analysis.rnn.sentiment_dataset import (\n",
    "    create_dummy_data,\n",
    "    download_data\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing a Text Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Smallville episode Justice is the best episode of Smallville ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! It's my favorite episode of Smallville! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "Label: 1\n",
      "\n",
      "Text: I don't know why I like this movie so well, but I never get tired of watching it.\n",
      "Label: 1\n",
      "\n",
      "Text: Smallville episode Justice is the best episode of Smallville ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! It's my favorite episode of Smallville! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "Label: 1\n",
      "\n",
      "Text: A rating of \"1\" does not begin to express how dull, depressing and relentlessly bad this movie is.\n",
      "Label: 0\n",
      "\n",
      "Text: Comment this movie is impossible. Is terrible, very improbable, bad interpretation e direction. Not look!!!!!\n",
      "Label: 0\n",
      "\n",
      "Text: I wouldn't rent this one even on dollar rental night.\n",
      "Label: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(path, \"datasets\", \"SentimentData\")\n",
    "path = download_data(data_root)\n",
    "data = create_dummy_data(path)\n",
    "for text, label in data:\n",
    "    print('Text: {}'.format(text))\n",
    "    print('Label: {}'.format(label))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tokenizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['smallville', 'episode', 'justice', 'is', 'the', 'best', 'episode', 'of', 'smallville', 'it', 's', 'my', 'favorite', 'episode', 'of', 'smallville'], 1) \n",
      "\n",
      "(['i', 'don', 't', 'know', 'why', 'i', 'like', 'this', 'movie', 'so', 'well', 'but', 'i', 'never', 'get', 'tired', 'of', 'watching', 'it'], 1) \n",
      "\n",
      "(['smallville', 'episode', 'justice', 'is', 'the', 'best', 'episode', 'of', 'smallville', 'it', 's', 'my', 'favorite', 'episode', 'of', 'smallville'], 1) \n",
      "\n",
      "(['a', 'rating', 'of', '1', 'does', 'not', 'begin', 'to', 'express', 'how', 'dull', 'depressing', 'and', 'relentlessly', 'bad', 'this', 'movie', 'is'], 0) \n",
      "\n",
      "(['comment', 'this', 'movie', 'is', 'impossible', 'is', 'terrible', 'very', 'improbable', 'bad', 'interpretation', 'e', 'direction', 'not', 'look'], 0) \n",
      "\n",
      "(['i', 'wouldn', 't', 'rent', 'this', 'one', 'even', 'on', 'dollar', 'rental', 'night'], 0) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    return [s.lower() for s in re.split(r'\\W+', text) if len(s) > 0]\n",
    "\n",
    "tokenized_data = []\n",
    "for text, label in data:\n",
    "    tokenized_data.append((tokenize(text), label))\n",
    "    print(tokenized_data[-1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Creating a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'smallville': 6,\n",
       "         'episode': 6,\n",
       "         'justice': 2,\n",
       "         'is': 5,\n",
       "         'the': 2,\n",
       "         'best': 2,\n",
       "         'of': 6,\n",
       "         'it': 3,\n",
       "         's': 2,\n",
       "         'my': 2,\n",
       "         'favorite': 2,\n",
       "         'i': 4,\n",
       "         'don': 1,\n",
       "         't': 2,\n",
       "         'know': 1,\n",
       "         'why': 1,\n",
       "         'like': 1,\n",
       "         'this': 4,\n",
       "         'movie': 3,\n",
       "         'so': 1,\n",
       "         'well': 1,\n",
       "         'but': 1,\n",
       "         'never': 1,\n",
       "         'get': 1,\n",
       "         'tired': 1,\n",
       "         'watching': 1,\n",
       "         'a': 1,\n",
       "         'rating': 1,\n",
       "         '1': 1,\n",
       "         'does': 1,\n",
       "         'not': 2,\n",
       "         'begin': 1,\n",
       "         'to': 1,\n",
       "         'express': 1,\n",
       "         'how': 1,\n",
       "         'dull': 1,\n",
       "         'depressing': 1,\n",
       "         'and': 1,\n",
       "         'relentlessly': 1,\n",
       "         'bad': 2,\n",
       "         'comment': 1,\n",
       "         'impossible': 1,\n",
       "         'terrible': 1,\n",
       "         'very': 1,\n",
       "         'improbable': 1,\n",
       "         'interpretation': 1,\n",
       "         'e': 1,\n",
       "         'direction': 1,\n",
       "         'look': 1,\n",
       "         'wouldn': 1,\n",
       "         'rent': 1,\n",
       "         'one': 1,\n",
       "         'even': 1,\n",
       "         'on': 1,\n",
       "         'dollar': 1,\n",
       "         'rental': 1,\n",
       "         'night': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "freqs = Counter()\n",
    "for tokens, _ in tokenized_data:\n",
    "    freqs.update(tokens)\n",
    "\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<eos>': 0,\n",
       " '<unk>': 1,\n",
       " 'smallville': 2,\n",
       " 'episode': 3,\n",
       " 'of': 4,\n",
       " 'is': 5,\n",
       " 'i': 6,\n",
       " 'this': 7,\n",
       " 'it': 8,\n",
       " 'movie': 9,\n",
       " 'justice': 10,\n",
       " 'the': 11,\n",
       " 'best': 12,\n",
       " 's': 13,\n",
       " 'my': 14,\n",
       " 'favorite': 15,\n",
       " 't': 16,\n",
       " 'not': 17,\n",
       " 'bad': 18,\n",
       " 'don': 19,\n",
       " 'know': 20,\n",
       " 'why': 21}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {'<eos>': 0, '<unk>': 1}\n",
    "for token, freq in freqs.most_common(20):\n",
    "    vocab[token] = len(vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 10, 5, 11, 12, 3, 4, 2, 8, 13, 14, 15, 3, 4, 2]  ->  1\n",
      "\n",
      "[6, 19, 16, 20, 21, 6, 1, 7, 9, 1, 1, 1, 6, 1, 1, 1, 4, 1, 8]  ->  1\n",
      "\n",
      "[2, 3, 10, 5, 11, 12, 3, 4, 2, 8, 13, 14, 15, 3, 4, 2]  ->  1\n",
      "\n",
      "[1, 1, 4, 1, 1, 17, 1, 1, 1, 1, 1, 1, 1, 1, 18, 7, 9, 5]  ->  0\n",
      "\n",
      "[1, 7, 9, 5, 1, 5, 1, 1, 1, 18, 1, 1, 1, 17, 1]  ->  0\n",
      "\n",
      "[6, 1, 16, 1, 7, 1, 1, 1, 1, 1, 1]  ->  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_data = []\n",
    "for tokens, label in tokenized_data:\n",
    "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    indexed_data.append((indices, label))\n",
    "\n",
    "\n",
    "for indices, label in indexed_data:\n",
    "    print(indices, ' -> ', label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': tensor([ 6, 19, 16, 20, 21,  6,  1,  7,  9,  1,  1,  1,  6,  1,  1,  1,  4,  1,\n",
      "         8]), 'label': tensor(1.)}\n",
      "\n",
      "{'data': tensor([ 1,  1,  4,  1,  1, 17,  1,  1,  1,  1,  1,  1,  1,  1, 18,  7,  9,  5]), 'label': tensor(0.)}\n",
      "\n",
      "{'data': tensor([ 2,  3, 10,  5, 11, 12,  3,  4,  2,  8, 13, 14, 15,  3,  4,  2]), 'label': tensor(1.)}\n",
      "\n",
      "{'data': tensor([ 2,  3, 10,  5, 11, 12,  3,  4,  2,  8, 13, 14, 15,  3,  4,  2]), 'label': tensor(1.)}\n",
      "\n",
      "{'data': tensor([ 1,  7,  9,  5,  1,  5,  1,  1,  1, 18,  1,  1,  1, 17,  1]), 'label': tensor(0.)}\n",
      "\n",
      "{'data': tensor([ 6,  1, 16,  1,  7,  1,  1,  1,  1,  1,  1]), 'label': tensor(0.)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentiment_analysis.rnn.sentiment_dataset import SentimentDataset\n",
    "\n",
    "combined_data = [\n",
    "    (raw_text, tokens, indices, label)\n",
    "    for (raw_text, label), (tokens, _), (indices, _)\n",
    "    in zip(data, tokenized_data, indexed_data)\n",
    "]\n",
    "\n",
    "dataset = SentimentDataset(combined_data)\n",
    "\n",
    "for elem in dataset:\n",
    "    print(elem)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Minibatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: \n",
      " tensor([[ 6,  1,  2],\n",
      "        [19,  1,  3],\n",
      "        [16,  4, 10],\n",
      "        [20,  1,  5],\n",
      "        [21,  1, 11],\n",
      "        [ 6, 17, 12],\n",
      "        [ 1,  1,  3],\n",
      "        [ 7,  1,  4],\n",
      "        [ 9,  1,  2],\n",
      "        [ 1,  1,  8],\n",
      "        [ 1,  1, 13],\n",
      "        [ 1,  1, 14],\n",
      "        [ 6,  1, 15],\n",
      "        [ 1,  1,  3],\n",
      "        [ 1, 18,  4],\n",
      "        [ 1,  7,  2],\n",
      "        [ 4,  9,  0],\n",
      "        [ 1,  5,  0],\n",
      "        [ 8,  0,  0]])\n",
      "\n",
      "Labels: \n",
      " tensor([1., 0., 1.])\n",
      "\n",
      "Sequence Lengths: \n",
      " tensor([19, 18, 16])\n",
      "\n",
      "\n",
      "Data: \n",
      " tensor([[ 2,  1,  6],\n",
      "        [ 3,  7,  1],\n",
      "        [10,  9, 16],\n",
      "        [ 5,  5,  1],\n",
      "        [11,  1,  7],\n",
      "        [12,  5,  1],\n",
      "        [ 3,  1,  1],\n",
      "        [ 4,  1,  1],\n",
      "        [ 2,  1,  1],\n",
      "        [ 8, 18,  1],\n",
      "        [13,  1,  1],\n",
      "        [14,  1,  0],\n",
      "        [15,  1,  0],\n",
      "        [ 3, 17,  0],\n",
      "        [ 4,  1,  0],\n",
      "        [ 2,  0,  0]])\n",
      "\n",
      "Labels: \n",
      " tensor([1., 0., 0.])\n",
      "\n",
      "Sequence Lengths: \n",
      " tensor([16, 15, 11])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(batch):\n",
    "    assert isinstance(batch, list)\n",
    "    data = pad_sequence([b['data'] for b in batch])\n",
    "    lengths = torch.tensor([len(b['data']) for b in batch])\n",
    "    label = torch.stack([b['label'] for b in batch])\n",
    "    return {\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'lengths': lengths\n",
    "    }\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=3, collate_fn=collate)\n",
    "for batch in loader:\n",
    "    print('Data: \\n', batch['data'])\n",
    "    print('\\nLabels: \\n', batch['label'])\n",
    "    print('\\nSequence Lengths: \\n', batch['lengths'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embeddings\n",
    "\n",
    "<img src='https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg' width=80% height=80%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between outputs: 0.0\n",
      "Test passed :)!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from sentiment_analysis.rnn.rnn_nn import Embedding\n",
    "\n",
    "i2dl_embedding = Embedding(len(vocab), 16, padding_idx=0)\n",
    "pytorch_embedding = nn.Embedding(len(vocab), 16, padding_idx=0)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=len(dataset), collate_fn=collate)\n",
    "for batch in loader:\n",
    "    x = batch['data']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}